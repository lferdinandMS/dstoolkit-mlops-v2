name: Register Datasets

on:
  workflow_dispatch:

env:
  SUBSCRIPTION_ID: ${{ vars.SUBSCRIPTION_ID }}
  RESOURCE_GROUP_NAME: ${{ vars.RESOURCE_GROUP_NAME }}
  WORKSPACE_NAME: ${{ vars.WORKSPACE_NAME }}
  ARM_CLIENT_ID:  ${{ vars.ARM_CLIENT_ID }}
  ARM_TENANT_ID:  ${{ vars.ARM_TENANT_ID }}

jobs:
  run-registration:
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Azure login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.ARM_CLIENT_ID }}
          tenant-id: ${{ vars.ARM_TENANT_ID }}
          subscription-id: ${{ vars.SUBSCRIPTION_ID }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Install Python Dependencies
        shell: bash
        run: |
          python -m pip install --upgrade pip
          python -m pip install --upgrade -r .github/requirements/execute_job_requirements.txt

      - name: "Preflight: verify identity access to default datastore"
        shell: bash
        env:
          AZURE_STORAGE_AUTH_MODE: login
          RESOURCE_GROUP_NAME: ${{ env.RESOURCE_GROUP_NAME }}
          WORKSPACE_NAME: ${{ env.WORKSPACE_NAME }}
        run: |
          set -euo pipefail
          echo "Installing ML CLI extension (if missing)..."
          az extension add -n ml -y >/dev/null 2>&1 || az extension update -n ml -y >/dev/null 2>&1 || true

          echo "Resolving default blob datastore (workspaceblobstore)..."
          # Query root-level properties (not nested under .properties)
          ACCOUNT=$(az ml datastore show -g "$RESOURCE_GROUP_NAME" -w "$WORKSPACE_NAME" -n workspaceblobstore --query account_name -o tsv 2>/dev/null || true)
          CONTAINER=$(az ml datastore show -g "$RESOURCE_GROUP_NAME" -w "$WORKSPACE_NAME" -n workspaceblobstore --query container_name -o tsv 2>/dev/null || true)

          if [ -z "$ACCOUNT" ] || [ -z "$CONTAINER" ] || [ "$ACCOUNT" = "null" ] || [ "$CONTAINER" = "null" ]; then
            echo "ERROR: Could not resolve workspaceblobstore details (account/container)." >&2
            echo "Listing datastores for debugging..." >&2
            az ml datastore list -g "$RESOURCE_GROUP_NAME" -w "$WORKSPACE_NAME" -o table || true
            echo "Full details for workspaceblobstore (if exists):" >&2
            az ml datastore show -g "$RESOURCE_GROUP_NAME" -w "$WORKSPACE_NAME" -n workspaceblobstore -o json || true
            exit 1
          fi

          echo "Checking container access via AAD login... (acct=$ACCOUNT, container=$CONTAINER)"
          if ! az storage container show --name "$CONTAINER" --account-name "$ACCOUNT" --auth-mode login >/dev/null 2>&1; then
            echo "ERROR: Identity-based access to the workspace default datastore failed." >&2
            echo "Ensure the service principal has 'Storage Blob Data Contributor' on the workspace storage account." >&2
            exit 1
          fi
          echo "Preflight OK: AAD access to datastore container works."

      - name: "Canary: AAD upload and register data asset via CLI"
        if: ${{ false }}
        shell: bash
        env:
          AZURE_STORAGE_AUTH_MODE: login
          RESOURCE_GROUP_NAME: ${{ env.RESOURCE_GROUP_NAME }}
          WORKSPACE_NAME: ${{ env.WORKSPACE_NAME }}
        run: |
          set -euo pipefail
          # Resolve account/container again
          ACCOUNT=$(az ml datastore show -g "$RESOURCE_GROUP_NAME" -w "$WORKSPACE_NAME" -n workspaceblobstore --query account_name -o tsv)
          CONTAINER=$(az ml datastore show -g "$RESOURCE_GROUP_NAME" -w "$WORKSPACE_NAME" -n workspaceblobstore --query container_name -o tsv)

          ASSET_NAME="copilot_ping"
          TS=$(date +%s)
          DEST_PATH="copilot-ping/$TS"

          echo "Creating small test payload..."
          mkdir -p tmp_ping && echo "ok $(date -u +%FT%TZ)" > tmp_ping/readme.txt

          echo "Uploading with AAD (no keys)..."
          az storage blob upload-batch \
            --account-name "$ACCOUNT" \
            --auth-mode login \
            --destination "$CONTAINER/$DEST_PATH" \
            --source tmp_ping >/dev/null

          echo "Registering a uri_folder data asset pointing to the uploaded path..."
          az ml data create -g "$RESOURCE_GROUP_NAME" -w "$WORKSPACE_NAME" \
            --name "$ASSET_NAME" \
            --version "$TS" \
            --type uri_folder \
            --path "azureml://datastores/workspaceblobstore/paths/$DEST_PATH/" \
            -o table

          echo "Canary OK. Data asset $ASSET_NAME:$TS created."

      - name: "Bulk: upload and register datasets via CLI"
        shell: bash
        env:
          AZURE_STORAGE_AUTH_MODE: login
          RESOURCE_GROUP_NAME: ${{ env.RESOURCE_GROUP_NAME }}
          WORKSPACE_NAME: ${{ env.WORKSPACE_NAME }}
        run: |
          set -euo pipefail
          # Ensure jq is available
          if ! command -v jq >/dev/null 2>&1; then
            sudo apt-get update -y && sudo apt-get install -y jq
          fi

          ACCOUNT=$(az ml datastore show -g "$RESOURCE_GROUP_NAME" -w "$WORKSPACE_NAME" -n workspaceblobstore --query account_name -o tsv)
          CONTAINER=$(az ml datastore show -g "$RESOURCE_GROUP_NAME" -w "$WORKSPACE_NAME" -n workspaceblobstore --query container_name -o tsv)
          TS=$(date +%s)

          echo "Starting bulk upload + registration with version=$TS"
          while read -r row; do
            NAME=$(echo "$row" | jq -r '.DATASET_NAME')
            DESC=$(echo "$row" | jq -r '.DATASET_DESC')
            SRC=$(echo "$row" | jq -r '.DATA_PATH')
            DEST_PATH="datasets/$NAME/$TS"

            echo "\nUploading '$SRC' -> '$CONTAINER/$DEST_PATH'"
            az storage blob upload-batch \
              --account-name "$ACCOUNT" \
              --auth-mode login \
              --destination "$CONTAINER/$DEST_PATH" \
              --source "$SRC"

            echo "Registering data asset '$NAME' version '$TS'"
            az ml data create -g "$RESOURCE_GROUP_NAME" -w "$WORKSPACE_NAME" \
              --name "$NAME" \
              --version "$TS" \
              --type uri_folder \
              --path "azureml://datastores/workspaceblobstore/paths/$DEST_PATH/" \
              --description "$DESC" \
              -o table
          done < <(jq -c '.datasets[]' config/data_config.json)
          echo "\nBulk registration complete."

      - name: "Optional: ensure latest azure-ai-ml + identity libs"
        if: ${{ false }}
        shell: bash
        run: |
          set -euo pipefail
          python -c "import azure.ai.ml, sys; print('azure-ai-ml version:', azure.ai.ml.__version__)" || true
          # Keep sdk fresh to avoid SAS fallbacks; harmless if already latest
          python -m pip install --upgrade "azure-ai-ml" "azure-identity" "azure-storage-blob"

      - name: Execute Dataset Registration (Python) [disabled]
        if: ${{ false }}
        shell: bash
        env:
          AZURE_STORAGE_AUTH_MODE: login
        run: |
              python -m mlops.common.register_data_asset \
                  --data_config_path config/data_config.json
